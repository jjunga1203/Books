{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPKSU6Y5yyeJz18aDlN+fHa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjunga1203/Books/blob/master/chat_Kobert_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mxnet\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "yxfYehBi1LeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ★\n",
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vL72HBfgKb9V",
        "outputId": "f1c029d7-93a4-4f93-c82a-4774c4aebfe9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kobert_tokenizer\n",
            "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-6fl8ewvh/kobert-tokenizer_3b157ad9abd2450bbc5dc9f5c3c19201\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-6fl8ewvh/kobert-tokenizer_3b157ad9abd2450bbc5dc9f5c3c19201\n",
            "  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: kobert_tokenizer\n",
            "  Building wheel for kobert_tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert_tokenizer: filename=kobert_tokenizer-0.1-py3-none-any.whl size=4633 sha256=47692264c931d920c1b1976ff2c63eee1d423fea19459f175282070c0a2b1451\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mdih_d9c/wheels/e9/1a/3f/a864970e8a169c176befa3c4a1e07aa612f69195907a4045fe\n",
            "Successfully built kobert_tokenizer\n",
            "Installing collected packages: kobert_tokenizer\n",
            "Successfully installed kobert_tokenizer-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gluonnlp==0.8.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIn12i8UK2oX",
        "outputId": "e97109d7-6a34-4cda-e899-132020b6b3e9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gluonnlp==0.8.0 in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.8.0) (1.21.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.21.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sE9HC8L1LP79",
        "outputId": "048bbbe5-089e-4858-d97c-de0ec6ccd5b4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.21.3 in /usr/local/lib/python3.10/dist-packages (1.21.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook"
      ],
      "metadata": {
        "id": "J8bNof4jKPZX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c-J40EthKaj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ★ Hugging Face를 통한 모델 및 토크나이저 Import\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ],
      "metadata": {
        "id": "iE7fap5WKPc-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU 사용 시\n",
        "device = torch.device(\"cuda:0\")"
      ],
      "metadata": {
        "id": "eUT_49mPLy0M"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ★\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
        "# vocab = vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n",
        "\n",
        "# 어휘 관련 기능을 사용하여 vocab을 생성합니다.\n",
        "vocab = tokenizer.get_vocab()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mWVOyLxL_7l",
        "outputId": "b0732d33-7ddb-49c6-fee2-92e6bcd7f129"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
            "The class this function is called from is 'KoBERTTokenizer'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel('./감성대화말뭉치(최종데이터)_Training.xlsx', index_col=0)\n"
      ],
      "metadata": {
        "id": "V-Ec5mcVMBi1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot_df = df[['감정_대분류','사람문장1', '사람문장2', '사람문장3']]\n",
        "chatbot_df.columns = ['Emotion', 'Sentence1', 'Sentence2', 'Sentence3']"
      ],
      "metadata": {
        "id": "WS8fo3n8N1Tq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emot = {'불안':0, '분노':1, '상처':2, '슬픔':3, '당황':4, '기쁨':5}\n",
        "chatbot_df['Emotion'] = chatbot_df['Emotion'].map(emot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HlpK6vgN1XQ",
        "outputId": "77b103b9-4803-4b31-eff4-8cf266318ef9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-4f99096f4c28>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chatbot_df['Emotion'] = chatbot_df['Emotion'].map(emot)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_list = []\n",
        "for q1, q2, q3, label in zip(chatbot_df['Sentence1'], chatbot_df['Sentence2'], chatbot_df['Sentence3'], chatbot_df['Emotion'])  :\n",
        "    if q1 != 'NaN':\n",
        "        data = []\n",
        "        data.append(q1)\n",
        "        data.append(label)\n",
        "        data_list.append(data)\n",
        "    if q2 != 'NaN':\n",
        "        data = []\n",
        "        data.append(q2)\n",
        "        data.append(label)\n",
        "        data_list.append(data)\n",
        "    if q3 != 'NaN':\n",
        "        data = []\n",
        "        data.append(q3)\n",
        "        data.append(label)\n",
        "        data_list.append(data)"
      ],
      "metadata": {
        "id": "hELCtO55N1aq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "dataset_train, dataset_test = train_test_split(data_list, test_size=0.25, random_state=0)\n",
        "\n",
        "print(len(dataset_train)) # 59604 출력\n",
        "print(len(dataset_test)) # 19869 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejJxnsG3N1eB",
        "outputId": "3fc64dcc-9e33-4731-e4a6-e4dad84b4323"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "116167\n",
            "38723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "WhKEeGm3N_By"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Setting parameters\n",
        "max_len = 64\n",
        "batch_size = 1\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5"
      ],
      "metadata": {
        "id": "81ARL_6CN_IY"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize\n",
        "# tokenizer = get_tokenizer()\n",
        "# from gluonnlp.data import BERTSPTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "\n",
        "# 문장을 토크나이징하는 함수\n",
        "def tokenize_sentences(sentences, tokenizer, max_len):\n",
        "    # 토크나이저를 사용하여 문장을 토큰화하고 인코딩합니다.\n",
        "    encodings = tokenizer(sentences, truncation=True, padding=True, max_length=max_len)\n",
        "    return encodings\n",
        "\n",
        "class BERTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.sentences = [i[sent_idx] for i in dataset]\n",
        "        self.labels = [i[label_idx] for i in dataset]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 문장을 토큰화하고 인코딩합니다.\n",
        "        encoding = tokenize_sentences(self.sentences[idx], self.tokenizer, self.max_len)\n",
        "        label = torch.tensor(self.labels[idx])\n",
        "        return encoding, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# BERT 토크나이저를 가져옵니다.\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# BERTDataset을 생성합니다.\n",
        "data_train = BERTDataset(dataset_train, 0, 1, tokenizer, max_len)\n",
        "data_test = BERTDataset(dataset_test, 0, 1, tokenizer, max_len)\n",
        "\n",
        "# DataLoader를 생성합니다.\n",
        "train_dataloader = DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
        "test_dataloader = DataLoader(data_test, batch_size=batch_size, num_workers=5)\n"
      ],
      "metadata": {
        "id": "wuvqX2MqODqq"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnErxaDbSEWr",
        "outputId": "e80c8461-201f-4bd5-f5ef-2e8d705c3be5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x78b3d4d0b070>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=7,\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "\n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "\n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ],
      "metadata": {
        "id": "WvUtkKw9ODuE"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#정의한 모델 불러오기\n",
        "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
        "# Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc\n",
        "# train_dataloader"
      ],
      "metadata": {
        "id": "3y70A3XuODzi"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 모델을 학습 모드로 설정합니다.\n",
        "model.train()\n",
        "\n",
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "    train_total = 0\n",
        "    test_total = 0\n",
        "\n",
        "\n",
        "\n",
        "    # 훈련 데이터셋에 대한 반복문\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # GPU로 데이터를 전송합니다.\n",
        "        token_ids = token_ids.to(device)\n",
        "        segment_ids = segment_ids.to(device)\n",
        "        valid_length = valid_length.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        # 모델에 입력을 전달하여 예측값을 얻습니다.\n",
        "        out = model(token_ids, attention_mask=valid_length, token_type_ids=segment_ids)\n",
        "\n",
        "        # 손실을 계산합니다.\n",
        "        loss = loss_fn(out.logits, label)\n",
        "\n",
        "        # 역전파를 수행하여 그래디언트를 계산합니다.\n",
        "        loss.backward()\n",
        "\n",
        "        # 그래디언트 클리핑을 수행합니다.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "        # 가중치를 업데이트합니다.\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Learning rate schedule 업데이트\n",
        "\n",
        "        # 정확도를 계산합니다.\n",
        "        predicted_labels = torch.argmax(out.logits, dim=1)\n",
        "        train_acc += torch.sum(predicted_labels == label).item()\n",
        "        train_total += label.size(0)\n",
        "\n",
        "        # 일정 간격마다 결과를 출력합니다.\n",
        "        if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.item(), train_acc / train_total))\n",
        "\n",
        "    # 훈련 정확도를 출력합니다.\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / train_total))\n",
        "\n",
        "    # 평가 데이터셋에 대한 반복문\n",
        "    model.eval()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "        token_ids = token_ids.to(device)\n",
        "        segment_ids = segment_ids.to(device)\n",
        "        valid_length = valid_length.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model(token_ids, attention_mask=valid_length, token_type_ids=segment_ids)\n",
        "            predicted_labels = torch.argmax(out.logits, dim=1)\n",
        "            test_acc += torch.sum(predicted_labels == label).item()\n",
        "            test_total += label.size(0)\n",
        "\n",
        "    # 테스트 정확도를 출력합니다.\n",
        "    print(\"epoch {} test acc {}\".format(e+1, test_acc / test_total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "eSe2O8lcN_Li",
        "outputId": "fcc96bee-283f-47bf-c69e-be55f13f8e0b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 4, got 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-7a4e6b72a472>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# 훈련 데이터셋에 대한 반복문\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for batch_idx, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "#     print(\"Batch index:\", batch_idx)\n",
        "#     print(\"Token IDs:\", token_ids)\n",
        "#     print(\"Valid length:\", valid_length)\n",
        "#     print(\"Segment IDs:\", segment_ids)\n",
        "#     print(\"Label:\", label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "bR-xMBZmRpI8",
        "outputId": "7f57eb8c-b6de-4433-bed6-dc9867fc143d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 4, got 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-7c0a4b5d270f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batch index:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Token IDs:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Valid length:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Segment IDs:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JaAvOgRqYaqr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}